{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Imports",
   "id": "d87307cf661a6d5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datetime\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from typing import Dict, List, Any, Tuple, Optional"
   ],
   "id": "cc66fe2d0e419b9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Clinical Data Preprocessing",
   "id": "90fedf47b1205c91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def parse_dt(dt_str: Optional[str]) -> Optional[datetime.datetime]:\n",
    "    if dt_str is None:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.datetime.fromisoformat(dt_str)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def safe_min(values):\n",
    "    vals = [v for v in values if v is not None and not pd.isna(v)]\n",
    "    return min(vals) if vals else None\n",
    "\n",
    "def safe_max(values):\n",
    "    vals = [v for v in values if v is not None and not pd.isna(v)]\n",
    "    return max(vals) if vals else None\n",
    "\n",
    "def aggregate_patient(patient: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Aggregate a single TCGA-GBM-style patient JSON into flat features.\n",
    "    \"\"\"\n",
    "    out: Dict[str, Any] = {\"submitter_id\": patient.get(\"submitter_id\")}\n",
    "\n",
    "    # Survival outcome data\n",
    "    demo = patient.get(\"demographic\", {}) or {}\n",
    "    vital_status = demo.get(\"vital_status\")\n",
    "    days_to_death = demo.get(\"days_to_death\")\n",
    "    days_to_last_follow_up = None\n",
    "\n",
    "    diagnoses = patient.get(\"diagnoses\", []) or []\n",
    "    for d in diagnoses:\n",
    "        if d.get(\"days_to_last_follow_up\") is not None:\n",
    "            days_to_last_follow_up = d.get(\"days_to_last_follow_up\")\n",
    "            break\n",
    "\n",
    "    if vital_status == \"Dead\" and days_to_death is not None:\n",
    "        time = float(days_to_death)\n",
    "        event = 1\n",
    "    else:\n",
    "        time = float(days_to_last_follow_up) if days_to_last_follow_up is not None else np.nan\n",
    "        event = 0\n",
    "\n",
    "    out[\"survival_time\"] = time\n",
    "    # out[\"vital_status\"] = event # 1 if dead, 0 if alive\n",
    "\n",
    "    # Demographic data\n",
    "    out[\"age_at_index\"] = demo.get(\"age_at_index\")\n",
    "    out[\"sex_at_birth\"] = demo.get(\"sex_at_birth\")\n",
    "    out[\"race\"] = demo.get(\"race\")\n",
    "    out[\"ethnicity\"] = demo.get(\"ethnicity\")\n",
    "\n",
    "    # Diagnoses: choose “final baseline” diagnosis and track progression\n",
    "    diag_records = []\n",
    "    for d in diagnoses:\n",
    "        days_to_diag = d.get(\"days_to_diagnosis\")\n",
    "        cdt = parse_dt(d.get(\"created_datetime\"))\n",
    "        udt = parse_dt(d.get(\"updated_datetime\"))\n",
    "        diag_records.append((d, days_to_diag, cdt, udt))\n",
    "\n",
    "    diag_records.sort(key=lambda x: (\n",
    "        float(\"inf\") if x[1] is None else x[1],\n",
    "        datetime.datetime.min if x[2] is None else x[2],\n",
    "        datetime.datetime.min if x[3] is None else x[3],\n",
    "    ))\n",
    "\n",
    "    # Separate primary-like vs progression-like\n",
    "    primary_like = []\n",
    "    progression_like = []\n",
    "\n",
    "    for d, dt_d, cdt, udt in diag_records:\n",
    "        cl = (d.get(\"classification_of_tumor\") or \"\").lower()\n",
    "        if cl in (\"primary\", \"initial diagnosis\", \"\"):\n",
    "            primary_like.append((d, dt_d, cdt, udt))\n",
    "        elif cl in (\"progression\", \"recurrence\"):\n",
    "            progression_like.append((d, dt_d, cdt, udt))\n",
    "        else:\n",
    "            progression_like.append((d, dt_d, cdt, udt))\n",
    "\n",
    "    # Final baseline (most specific, latest among primary-like; fallback to earliest diag)\n",
    "    if primary_like:\n",
    "        # take latest in time among primary_like\n",
    "        primary_like.sort(key=lambda x: (\n",
    "            float(\"-inf\") if x[1] is None else x[1],\n",
    "            datetime.datetime.min if x[2] is None else x[2],\n",
    "            datetime.datetime.min if x[3] is None else x[3],\n",
    "        ), reverse=True)\n",
    "        base_diag = primary_like[0][0]\n",
    "    elif diag_records:\n",
    "        base_diag = diag_records[0][0]\n",
    "    else:\n",
    "        base_diag = {}\n",
    "\n",
    "    out[\"primary_site\"] = patient.get(\"primary_site\")\n",
    "    out[\"project_id\"] = patient.get(\"project\", {}).get(\"project_id\")\n",
    "\n",
    "    out[\"baseline_primary_diagnosis\"] = base_diag.get(\"primary_diagnosis\")\n",
    "    out[\"baseline_morphology\"] = base_diag.get(\"morphology\")\n",
    "    out[\"baseline_icd10\"] = base_diag.get(\"icd_10_code\")\n",
    "    out[\"baseline_classification\"] = base_diag.get(\"classification_of_tumor\")\n",
    "    out[\"baseline_year_of_diagnosis\"] = base_diag.get(\"year_of_diagnosis\")\n",
    "    out[\"baseline_age_at_diagnosis\"] = base_diag.get(\"age_at_diagnosis\")\n",
    "\n",
    "    # Progression diagnoses (optional; you can drop if you rely on follow_ups only)\n",
    "    prog_days = [\n",
    "        rec[1] for rec in progression_like\n",
    "        if rec[1] is not None\n",
    "    ]\n",
    "    out[\"num_progression_diagnoses\"] = len(prog_days)\n",
    "    out[\"diag_days_to_first_progression\"] = safe_min(prog_days) if prog_days else np.nan\n",
    "\n",
    "    # Disease course and performance status data\n",
    "    follow_ups = patient.get(\"follow_ups\", []) or []\n",
    "\n",
    "    days_to_progression_list = []\n",
    "    kps_values = []\n",
    "\n",
    "    for f in follow_ups:\n",
    "        prog_flag = f.get(\"progression_or_recurrence\")\n",
    "        days_to_prog = f.get(\"days_to_progression\")\n",
    "        if prog_flag == \"Yes\" and days_to_prog is not None:\n",
    "            days_to_progression_list.append(days_to_prog)\n",
    "\n",
    "        kps = f.get(\"karnofsky_performance_status\")\n",
    "        if kps is not None and kps != \"Unknown\":\n",
    "            kps_values.append(float(kps))\n",
    "\n",
    "    # First progression from follow-up data\n",
    "    out[\"first_progression\"] = 1 if days_to_progression_list else 0\n",
    "    out[\"days_to_first_progression\"] = float(min(days_to_progression_list)) if days_to_progression_list else np.nan\n",
    "    out[\"num_progression_events\"] = len(days_to_progression_list) if days_to_progression_list else 0\n",
    "\n",
    "    out[\"kps_min\"] = safe_min(kps_values)\n",
    "    out[\"kps_max\"] = safe_max(kps_values)\n",
    "    out[\"kps_mean\"] = float(np.mean(kps_values)) if kps_values else np.nan\n",
    "\n",
    "    # Treatment summary data\n",
    "    all_treatments = []\n",
    "    for d in diagnoses:\n",
    "        for t in d.get(\"treatments\", []) or []:\n",
    "            all_treatments.append(t)\n",
    "\n",
    "    def any_treat(filter_fn):\n",
    "        return any(filter_fn(t) for t in all_treatments) # True if a given treatment was performed, False otherwise\n",
    "\n",
    "    def earliest_start(filter_fn):\n",
    "        vals = [\n",
    "            t.get(\"days_to_treatment_start\")\n",
    "            for t in all_treatments\n",
    "            if filter_fn(t) and t.get(\"days_to_treatment_start\") is not None\n",
    "        ]\n",
    "        return safe_min(vals)\n",
    "\n",
    "    def total_dose(filter_fn):\n",
    "        vals = [\n",
    "            t.get(\"treatment_dose\")\n",
    "            for t in all_treatments\n",
    "            if filter_fn(t) and t.get(\"treatment_dose\") is not None\n",
    "        ]\n",
    "        return float(np.nansum(vals)) if vals else np.nan\n",
    "\n",
    "    def is_surgery(t):\n",
    "        return \"surgery\" in (t.get(\"treatment_type\") or \"\").lower()\n",
    "\n",
    "    def is_radiation(t):\n",
    "        return \"radiation\" in (t.get(\"treatment_type\") or \"\").lower()\n",
    "\n",
    "    def is_chemo(t):\n",
    "        return \"chemo\" in (t.get(\"treatment_type\") or \"\").lower()\n",
    "\n",
    "    def is_dexamethasone(t):\n",
    "        agents = (t.get(\"therapeutic_agents\") or \"\").lower()\n",
    "        return \"dexamethasone\" in agents\n",
    "\n",
    "    out[\"any_surgery\"] = int(any_treat(is_surgery))\n",
    "    out[\"days_to_first_surgery\"] = earliest_start(is_surgery)\n",
    "    out[\"any_radiation\"] = int(any_treat(is_radiation))\n",
    "    out[\"days_to_first_radiation\"] = earliest_start(is_radiation)\n",
    "\n",
    "    rt_doses = [\n",
    "        t.get(\"treatment_dose\")\n",
    "        for t in all_treatments\n",
    "        if is_radiation(t) and t.get(\"treatment_dose\") is not None\n",
    "    ]\n",
    "    rt_fracs = [\n",
    "        t.get(\"number_of_fractions\")\n",
    "        for t in all_treatments\n",
    "        if is_radiation(t) and t.get(\"number_of_fractions\") is not None\n",
    "    ]\n",
    "\n",
    "    out[\"rt_total_dose\"] = float(np.nansum(rt_doses)) if rt_doses else np.nan\n",
    "    out[\"rt_total_fractions\"] = float(np.nansum(rt_fracs)) if rt_fracs else np.nan\n",
    "\n",
    "    out[\"any_chemo\"] = int(any_treat(is_chemo))\n",
    "    out[\"days_to_first_chemo\"] = earliest_start(is_chemo)\n",
    "    out[\"chemo_total_dose\"] = total_dose(is_chemo)\n",
    "\n",
    "    out[\"any_dexamethasone\"] = int(any_treat(is_dexamethasone))\n",
    "    out[\"days_to_first_dexamethasone\"] = earliest_start(is_dexamethasone)\n",
    "\n",
    "    return out, time, event\n",
    "\n",
    "def aggregate_cohort(patients: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    features_list = []\n",
    "    labels_dict = {}\n",
    "\n",
    "    for p in patients:\n",
    "        features, survival_time, survival_event = aggregate_patient(p)\n",
    "        features_list.append(features)\n",
    "\n",
    "        pid = p.get(\"submitter_id\")\n",
    "        if pd.notna(survival_time):\n",
    "            labels_dict[pid] = (survival_time, survival_event)\n",
    "\n",
    "    df = pd.DataFrame(features_list)\n",
    "    nunique = df.nunique(dropna=False)\n",
    "    constant_cols = nunique[nunique <= 1].index.tolist()\n",
    "    df = df.drop(columns=constant_cols)\n",
    "\n",
    "    return df, labels_dict\n",
    "\n",
    "class ClinicalDataProcessor:\n",
    "    \"\"\"\n",
    "    Processes clinical data from TCGA-style JSON into model-ready format\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 categorical_threshold: int = 10,\n",
    "                 imputation_strategy: str = 'median',\n",
    "                 normalize_numerical: bool = True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            categorical_threshold: Max unique values for a column to be treated as categorical\n",
    "            imputation_strategy: 'mean', 'median', or 'most_frequent'\n",
    "            normalize_numerical: Whether to standardize numerical features\n",
    "        \"\"\"\n",
    "        self.categorical_threshold = categorical_threshold\n",
    "        self.imputation_strategy = imputation_strategy\n",
    "        self.normalize_numerical = normalize_numerical\n",
    "        self.preprocessor = None\n",
    "        self.feature_names = None\n",
    "        self.numerical_cols = None\n",
    "        self.categorical_cols = None\n",
    "        self.id_col = 'submitter_id'\n",
    "\n",
    "    def _identify_column_types(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:\n",
    "        numerical_cols = []\n",
    "        categorical_cols = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            if col == self.id_col:\n",
    "                continue\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                # Check if actually categorical despite numeric dtype\n",
    "                unique_vals = df[col].nunique(dropna=True)\n",
    "                if unique_vals <= self.categorical_threshold and unique_vals > 1:\n",
    "                    categorical_cols.append(col)\n",
    "                else:\n",
    "                    numerical_cols.append(col)\n",
    "            else:\n",
    "                categorical_cols.append(col)\n",
    "\n",
    "        return numerical_cols, categorical_cols\n",
    "\n",
    "    def preprocess_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        processed = df.copy()\n",
    "        ids = processed[self.id_col].reset_index(drop=True)\n",
    "        processed = processed.drop(columns=[self.id_col])\n",
    "        self.numerical_cols, self.categorical_cols = self._identify_column_types(processed)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            processed[col] = processed[col].astype(str).replace('nan', 'missing')\n",
    "\n",
    "        for col in self.numerical_cols:\n",
    "            processed[col] = pd.to_numeric(processed[col], errors='coerce')\n",
    "            processed[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        transformers = []\n",
    "        if self.numerical_cols:\n",
    "            num_pipeline_steps = [\n",
    "                ('imputer', SimpleImputer(strategy=self.imputation_strategy))\n",
    "            ]\n",
    "            if self.normalize_numerical:\n",
    "                num_pipeline_steps.append(('scaler', StandardScaler()))\n",
    "\n",
    "            num_pipeline = Pipeline(num_pipeline_steps)\n",
    "            transformers.append(('num', num_pipeline, self.numerical_cols))\n",
    "\n",
    "        if self.categorical_cols:\n",
    "            cat_pipeline = Pipeline([\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "            ])\n",
    "            transformers.append(('cat', cat_pipeline, self.categorical_cols))\n",
    "\n",
    "        self.preprocessor = ColumnTransformer(transformers, remainder='drop')\n",
    "        processed_array = self.preprocessor.fit_transform(processed)\n",
    "        self.feature_names = []\n",
    "\n",
    "        if self.numerical_cols:\n",
    "            if self.normalize_numerical:\n",
    "                self.feature_names.extend(self.numerical_cols)\n",
    "            else:\n",
    "                self.feature_names.extend([f\"{col}_imputed\" for col in self.numerical_cols])\n",
    "\n",
    "        if self.categorical_cols:\n",
    "            cat_transformer = self.preprocessor.named_transformers_['cat']\n",
    "            encoder = cat_transformer.named_steps['encoder']\n",
    "\n",
    "            encoded_names = encoder.get_feature_names_out(self.categorical_cols)\n",
    "            self.feature_names.extend(encoded_names)\n",
    "\n",
    "        result_df = pd.DataFrame(processed_array, columns=self.feature_names)\n",
    "        result_df[self.id_col] = ids\n",
    "        cols = [self.id_col] + [c for c in result_df.columns if c != self.id_col]\n",
    "        result_df = result_df[cols]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def transform_new_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if self.preprocessor is None:\n",
    "            raise ValueError(\"Preprocessor not fitted. Call preprocess_dataframe first.\")\n",
    "\n",
    "        processed = df.copy()\n",
    "        ids = processed[self.id_col].reset_index(drop=True)\n",
    "        processed = processed.drop(columns=[self.id_col])\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in processed.columns:\n",
    "                processed[col] = processed[col].astype(str).replace('nan', 'missing')\n",
    "\n",
    "        for col in self.numerical_cols:\n",
    "            if col in processed.columns:\n",
    "                processed[col] = pd.to_numeric(processed[col], errors='coerce')\n",
    "                processed[col].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "        processed_array = self.preprocessor.transform(processed)\n",
    "\n",
    "        result_df = pd.DataFrame(processed_array, columns=self.feature_names)\n",
    "        result_df[self.id_col] = ids\n",
    "        cols = [self.id_col] + [c for c in result_df.columns if c != self.id_col]\n",
    "        result_df = result_df[cols]\n",
    "\n",
    "        return result_df\n",
    "\n",
    "    def build_feature_mapping(self, df: pd.DataFrame) -> Tuple[Dict[str, np.ndarray], List[str]]:\n",
    "        if self.id_col not in df.columns:\n",
    "            raise ValueError(f\"DataFrame must contain '{self.id_col}' column\")\n",
    "\n",
    "        mapping = {}\n",
    "        for idx, row in df.iterrows():\n",
    "            patient_id = row[self.id_col]\n",
    "            features = row.drop(self.id_col).astype(float).to_numpy()\n",
    "            mapping[patient_id] = features\n",
    "\n",
    "        return mapping, self.feature_names\n",
    "\n",
    "    def save_processor(self, filepath: str):\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'preprocessor': self.preprocessor,\n",
    "                'feature_names': self.feature_names,\n",
    "                'numerical_cols': self.numerical_cols,\n",
    "                'categorical_cols': self.categorical_cols,\n",
    "                'id_col': self.id_col,\n",
    "                'params': {\n",
    "                    'categorical_threshold': self.categorical_threshold,\n",
    "                    'imputation_strategy': self.imputation_strategy,\n",
    "                    'normalize_numerical': self.normalize_numerical\n",
    "                }\n",
    "            }, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_processor(cls, filepath: str) -> 'ClinicalDataProcessor':\n",
    "        \"\"\"Load a fitted processor\"\"\"\n",
    "        with open(filepath, 'rb') as f:\n",
    "            saved = pickle.load(f)\n",
    "\n",
    "        processor = cls(**saved['params'])\n",
    "        processor.preprocessor = saved['preprocessor']\n",
    "        processor.feature_names = saved['feature_names']\n",
    "        processor.numerical_cols = saved['numerical_cols']\n",
    "        processor.categorical_cols = saved['categorical_cols']\n",
    "        processor.id_col = saved['id_col']\n",
    "\n",
    "        return processor\n",
    "\n",
    "def process_clinical_data(patient_json_list: List[Dict[str, Any]],\n",
    "                          output_dir: str = \"data\",\n",
    "                          save_intermediate: bool = True) -> Dict[str, np.ndarray]:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df_raw, survival_labels = aggregate_cohort(patient_json_list)\n",
    "\n",
    "    if save_intermediate:\n",
    "        df_raw.to_csv(f\"{output_dir}/clinical_raw.csv\", index=False)\n",
    "        with open(f\"{output_dir}/survival_labels.pkl\", \"wb\") as f:\n",
    "            pickle.dump(survival_labels, f)\n",
    "\n",
    "    processor = ClinicalDataProcessor(\n",
    "        categorical_threshold=10,\n",
    "        imputation_strategy='median',\n",
    "        normalize_numerical=True\n",
    "    )\n",
    "\n",
    "    df_processed = processor.preprocess_dataframe(df_raw)\n",
    "    feature_mapping, feature_names = processor.build_feature_mapping(df_processed)\n",
    "\n",
    "    with open(f\"{output_dir}/clinical_map.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"mapping\": feature_mapping,\n",
    "            \"feature_names\": feature_names,\n",
    "            \"patient_ids\": list(feature_mapping.keys()),\n",
    "            \"n_features\": len(feature_names)\n",
    "        }, f)\n",
    "\n",
    "    with open(f\"{output_dir}/survival_labels.pkl\", \"wb\") as f:\n",
    "        pickle.dump(survival_labels, f)\n",
    "    processor.save_processor(f\"{output_dir}/clinical_processor.pkl\")\n",
    "    df_processed.to_csv(f\"{output_dir}/clinical_processed.csv\", index=False)\n",
    "\n",
    "    feature_summary = pd.DataFrame({\n",
    "        'feature_name': feature_names,\n",
    "        'feature_type': ['numerical' if name in processor.numerical_cols\n",
    "                        else 'categorical_encoded' for name in feature_names]\n",
    "    })\n",
    "    feature_summary.to_csv(f\"{output_dir}/clinical_features.csv\", index=False)\n",
    "    return feature_mapping, survival_labels"
   ],
   "id": "a6b87a4e60601687"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "with open(\"../../PathomicFusion/data/TCGA_GBMLGG/clinical.project-tcga-gbm.2025-11-26.json\", \"r\") as f:\n",
    "    patient_json_list = json.load(f)\n",
    "\n",
    "# Process the data\n",
    "clinical_mapping, survival_labels = process_clinical_data(patient_json_list, output_dir=\"../../PathomicFusion/data/TCGA_GBMLGG\")"
   ],
   "id": "20777c664ace104a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing",
   "id": "3c804ac4dfa7a5d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class ClinicalEmbeddingAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze clinical embeddings for meaningfulness\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, clinical_map_path: str = \"data/clinical_map.pkl\"):\n",
    "        with open(clinical_map_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        self.mapping = data[\"mapping\"]\n",
    "        self.feature_names = data[\"feature_names\"]\n",
    "        self.patient_ids = list(self.mapping.keys())\n",
    "        self.X = np.stack([self.mapping[pid] for pid in self.patient_ids])\n",
    "        self.n_patients, self.n_features = self.X.shape\n",
    "\n",
    "        print(f\"Loaded {self.n_patients} patients with {self.n_features} features\")\n",
    "\n",
    "    def basic_statistics(self):\n",
    "        print(\"=\" * 60)\n",
    "        print(\"BASIC EMBEDDING STATISTICS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        stats_dict = {\n",
    "            \"Shape\": f\"{self.n_patients} patients × {self.n_features} features\",\n",
    "            \"Sparsity (%)\": f\"{100 * (self.X == 0).sum() / self.X.size:.1f}\",\n",
    "            \"NaN values\": f\"{np.isnan(self.X).sum()}\",\n",
    "            \"Mean ± Std\": f\"{self.X.mean():.3f} ± {self.X.std():.3f}\",\n",
    "            \"Min/Max\": f\"[{self.X.min():.3f}, {self.X.max():.3f}]\",\n",
    "            \"Feature correlation (mean)\": f\"{np.abs(np.corrcoef(self.X.T)).mean():.3f}\",\n",
    "        }\n",
    "\n",
    "        for key, value in stats_dict.items():\n",
    "            print(f\"{key:30}: {value}\")\n",
    "\n",
    "        feature_stds = self.X.std(axis=0)\n",
    "        constant_features = np.sum(feature_stds < 1e-10)\n",
    "        print(f\"{'Constant features':30}: {constant_features}\")\n",
    "\n",
    "        if constant_features > 0:\n",
    "            print(f\"Warning: {constant_features} features are constant!\")\n",
    "\n",
    "        # Feature value distribution\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(131)\n",
    "        plt.hist(self.X.flatten(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Feature value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title('Overall value distribution')\n",
    "\n",
    "        plt.subplot(132)\n",
    "        plt.boxplot(self.X.T)\n",
    "        plt.ylabel('Feature value')\n",
    "        plt.title('Feature-wise distributions')\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(self.X[:50, :], aspect='auto', cmap='RdBu_r',\n",
    "                  vmin=-3, vmax=3)\n",
    "        plt.colorbar(label='Feature value')\n",
    "        plt.xlabel('Feature index')\n",
    "        plt.ylabel('Patient index')\n",
    "        plt.title('First 50 patients (heatmap)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_embeddings(self, survival_data=None):\n",
    "        \"\"\"\n",
    "        Visualize embeddings in 2D/3D using dimensionality reduction\n",
    "\n",
    "        Args:\n",
    "            survival_data: Optional dict {patient_id: (time, event)} for coloring\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"VISUALIZING EMBEDDINGS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Prepare labels if available\n",
    "        colors = None\n",
    "        labels = None\n",
    "\n",
    "        if survival_data:\n",
    "            colors = []\n",
    "            for pid in self.patient_ids:\n",
    "                if pid in survival_data:\n",
    "                    time, event = survival_data[pid]\n",
    "                    colors.append('red' if event == 1 else 'blue')\n",
    "                else:\n",
    "                    colors.append('gray')\n",
    "            colors = np.array(colors)\n",
    "\n",
    "            # Count events\n",
    "            events = sum(1 for pid in self.patient_ids\n",
    "                        if pid in survival_data and survival_data[pid][1] == 1)\n",
    "            print(f\"Survival data: {events} events, {len(colors)-events} censored\")\n",
    "\n",
    "        # Reduce dimensions\n",
    "        methods = {\n",
    "            'PCA (2D)': PCA(n_components=2),\n",
    "            't-SNE': TSNE(n_components=2, perplexity=30, random_state=42),\n",
    "            'UMAP': umap.UMAP(n_components=2, random_state=42)\n",
    "        }\n",
    "\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        for ax, (name, reducer) in zip(axes, methods.items()):\n",
    "            try:\n",
    "                X_reduced = reducer.fit_transform(self.X)\n",
    "\n",
    "                if colors is not None:\n",
    "                    # Color by survival status\n",
    "                    for color in ['red', 'blue', 'gray']:\n",
    "                        mask = colors == color\n",
    "                        if mask.any():\n",
    "                            label = 'Dead' if color == 'red' else \\\n",
    "                                   'Alive' if color == 'blue' else 'Unknown'\n",
    "                            ax.scatter(X_reduced[mask, 0], X_reduced[mask, 1],\n",
    "                                     c=color, s=30, alpha=0.6, label=label)\n",
    "                    ax.legend()\n",
    "                else:\n",
    "                    ax.scatter(X_reduced[:, 0], X_reduced[:, 1],\n",
    "                             c='blue', s=30, alpha=0.6)\n",
    "\n",
    "                ax.set_title(f'{name}\\nVariance: {reducer.explained_variance_ratio_.sum():.2%}'\n",
    "                           if hasattr(reducer, 'explained_variance_ratio_') else name)\n",
    "                ax.set_xlabel('Component 1')\n",
    "                ax.set_ylabel('Component 2')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "\n",
    "            except Exception as e:\n",
    "                ax.text(0.5, 0.5, f'Error:\\n{str(e)}',\n",
    "                       ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(f'{name} (Failed)')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 3D visualization with PCA\n",
    "        try:\n",
    "            pca_3d = PCA(n_components=3)\n",
    "            X_3d = pca_3d.fit_transform(self.X)\n",
    "\n",
    "            fig = plt.figure(figsize=(10, 8))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "            if colors is not None:\n",
    "                for color in ['red', 'blue', 'gray']:\n",
    "                    mask = colors == color\n",
    "                    if mask.any():\n",
    "                        label = 'Dead' if color == 'red' else \\\n",
    "                               'Alive' if color == 'blue' else 'Unknown'\n",
    "                        ax.scatter(X_3d[mask, 0], X_3d[mask, 1], X_3d[mask, 2],\n",
    "                                 c=color, s=30, alpha=0.6, label=label)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                ax.scatter(X_3d[:, 0], X_3d[:, 1], X_3d[:, 2],\n",
    "                         c='blue', s=30, alpha=0.6)\n",
    "\n",
    "            ax.set_title(f'3D PCA (Total variance: {pca_3d.explained_variance_ratio_.sum():.2%})')\n",
    "            ax.set_xlabel('PC1')\n",
    "            ax.set_ylabel('PC2')\n",
    "            ax.set_zlabel('PC3')\n",
    "\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"3D visualization failed: {e}\")\n",
    "\n",
    "    def cluster_analysis(self):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CLUSTER ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        pca = PCA(n_components=min(50, self.n_features))\n",
    "        X_pca = pca.fit_transform(self.X)\n",
    "\n",
    "        # Determine optimal number of clusters\n",
    "        inertias = []\n",
    "        silhouette_scores = []\n",
    "        k_range = range(2, min(10, self.n_patients // 10))\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "            inertias.append(kmeans.inertia_)\n",
    "            if len(np.unique(labels)) > 1:\n",
    "                silhouette_scores.append(silhouette_score(X_pca, labels))\n",
    "            else:\n",
    "                silhouette_scores.append(0)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        axes[0].plot(k_range, inertias, 'bo-')\n",
    "        axes[0].set_xlabel('Number of clusters')\n",
    "        axes[0].set_ylabel('Inertia')\n",
    "        axes[0].set_title('Elbow Method')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        axes[1].plot(k_range, silhouette_scores, 'ro-')\n",
    "        axes[1].set_xlabel('Number of clusters')\n",
    "        axes[1].set_ylabel('Silhouette Score')\n",
    "        axes[1].set_title('Silhouette Analysis')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Clustering with optimal k\n",
    "        optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"Optimal clusters (silhouette): {optimal_k}\")\n",
    "\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X_pca)\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(self.X)\n",
    "\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1],\n",
    "                            c=cluster_labels, cmap='tab10',\n",
    "                            s=50, alpha=0.6)\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.title(f'Clusters (t-SNE, k={optimal_k})')\n",
    "        plt.xlabel('t-SNE 1')\n",
    "        plt.ylabel('t-SNE 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nCluster sizes:\")\n",
    "        unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        for cluster, count in zip(unique, counts):\n",
    "            print(f\"  Cluster {cluster}: {count} patients ({count/self.n_patients:.1%})\")\n",
    "\n",
    "        if len(np.unique(cluster_labels)) > 1:\n",
    "            silhouette_avg = silhouette_score(X_pca, cluster_labels)\n",
    "            calinski_score = calinski_harabasz_score(X_pca, cluster_labels)\n",
    "            print(f\"\\nSilhouette score: {silhouette_avg:.3f}\")\n",
    "            print(f\"Calinski-Harabasz score: {calinski_score:.1f}\")\n",
    "\n",
    "            if silhouette_avg < 0.2:\n",
    "                print(\"Low silhouette score - clusters may not be well separated\")\n",
    "            elif silhouette_avg > 0.5:\n",
    "                print(\"Good silhouette score - clusters are reasonably separated\")\n",
    "\n",
    "    def feature_importance_analysis(self, survival_data=None):\n",
    "        \"\"\"\n",
    "        Analyze which features are most important\n",
    "\n",
    "        Args:\n",
    "            survival_data: Optional dict {patient_id: (time, event)} for survival analysis\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if survival_data is None:\n",
    "            # Without survival data, use variance as importance\n",
    "            variances = self.X.var(axis=0)\n",
    "            top_indices = np.argsort(variances)[-20:][::-1]\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.barh(range(20), variances[top_indices])\n",
    "            plt.yticks(range(20), [self.feature_names[i] for i in top_indices])\n",
    "            plt.xlabel('Variance')\n",
    "            plt.title('Top 20 Features by Variance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Top 10 features by variance:\")\n",
    "            for i in top_indices[:10]:\n",
    "                print(f\"  {self.feature_names[i]}: {variances[i]:.3f}\")\n",
    "\n",
    "        else:\n",
    "            # With survival data, compute Cox regression-like scores\n",
    "            from sklearn.feature_selection import f_classif\n",
    "\n",
    "            # Create binary labels from survival data\n",
    "            y = np.zeros(self.n_patients)\n",
    "            for i, pid in enumerate(self.patient_ids):\n",
    "                if pid in survival_data:\n",
    "                    _, event = survival_data[pid]\n",
    "                    y[i] = event\n",
    "\n",
    "            # Compute ANOVA F-values\n",
    "            f_values, p_values = f_classif(self.X, y)\n",
    "\n",
    "            # Plot\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            # Top features by F-value\n",
    "            top_n = min(20, len(f_values))\n",
    "            top_indices = np.argsort(f_values)[-top_n:][::-1]\n",
    "\n",
    "            axes[0].barh(range(top_n), f_values[top_indices])\n",
    "            axes[0].set_yticks(range(top_n))\n",
    "            axes[0].set_yticklabels([self.feature_names[i] for i in top_indices])\n",
    "            axes[0].set_xlabel('F-value')\n",
    "            axes[0].set_title(f'Top {top_n} Features for Survival Prediction')\n",
    "\n",
    "            # p-value distribution\n",
    "            significant = p_values < 0.05\n",
    "            axes[1].hist(p_values, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[1].axvline(0.05, color='red', linestyle='--', label='p=0.05')\n",
    "            axes[1].set_xlabel('p-value')\n",
    "            axes[1].set_ylabel('Count')\n",
    "            axes[1].set_title(f'p-value distribution\\n{significant.sum()}/{len(p_values)} significant (p<0.05)')\n",
    "            axes[1].legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(f\"\\nSurvival-related features:\")\n",
    "            print(f\"Significant features (p<0.05): {significant.sum()}/{len(p_values)}\")\n",
    "            print(\"\\nTop 10 survival-associated features:\")\n",
    "            for i in top_indices[:10]:\n",
    "                feat_name = self.feature_names[i]\n",
    "                if len(feat_name) > 50:\n",
    "                    feat_name = feat_name[:47] + \"...\"\n",
    "                print(f\"  {feat_name:50} F={f_values[i]:6.2f}, p={p_values[i]:.2e}\")\n",
    "\n",
    "    def correlation_structure(self):\n",
    "        \"\"\"Analyze correlation structure of features\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CORRELATION STRUCTURE\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Compute correlation matrix\n",
    "        corr_matrix = np.corrcoef(self.X.T)\n",
    "\n",
    "        # Plot correlation matrix\n",
    "        plt.figure(figsize=(12, 10))\n",
    "\n",
    "        # For large matrices, sample features\n",
    "        if self.n_features > 50:\n",
    "            # Select top 50 features by variance\n",
    "            variances = self.X.var(axis=0)\n",
    "            top_indices = np.argsort(variances)[-50:][::-1]\n",
    "            sampled_corr = corr_matrix[top_indices][:, top_indices]\n",
    "            feature_labels = [self.feature_names[i] for i in top_indices]\n",
    "\n",
    "            # Truncate long labels\n",
    "            feature_labels = [lab[:30] + \"...\" if len(lab) > 30 else lab\n",
    "                            for lab in feature_labels]\n",
    "        else:\n",
    "            sampled_corr = corr_matrix\n",
    "            feature_labels = self.feature_names\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.heatmap(sampled_corr,\n",
    "                   cmap='RdBu_r',\n",
    "                   center=0,\n",
    "                   square=True,\n",
    "                   xticklabels=feature_labels,\n",
    "                   yticklabels=feature_labels)\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Analyze correlation distribution\n",
    "        upper_tri = np.triu_indices_from(corr_matrix, k=1)\n",
    "        correlations = corr_matrix[upper_tri]\n",
    "\n",
    "        plt.figure(figsize=(10, 4))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.hist(correlations, bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Correlation coefficient')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of feature correlations')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.hist(np.abs(correlations), bins=50, alpha=0.7, edgecolor='black')\n",
    "        plt.xlabel('Absolute correlation')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Distribution of absolute correlations')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Correlation statistics:\")\n",
    "        print(f\"  Mean absolute correlation: {np.abs(correlations).mean():.3f}\")\n",
    "        print(f\"  High correlations (|r|>0.7): {(np.abs(correlations) > 0.7).sum()}\")\n",
    "        print(f\"  Moderate correlations (0.3<|r|<0.7): {((np.abs(correlations) > 0.3) & (np.abs(correlations) < 0.7)).sum()}\")\n",
    "        print(f\"  Low correlations (|r|<0.1): {(np.abs(correlations) < 0.1).sum()}\")\n",
    "\n",
    "        # Check for highly correlated features (potential redundancy)\n",
    "        high_corr_pairs = np.where(np.abs(corr_matrix) > 0.9)\n",
    "        high_corr_pairs = [(i, j, corr_matrix[i, j])\n",
    "                          for i, j in zip(*high_corr_pairs)\n",
    "                          if i < j]\n",
    "\n",
    "        if high_corr_pairs:\n",
    "            print(f\"\\nFound {len(high_corr_pairs)} highly correlated feature pairs (|r|>0.9):\")\n",
    "            for i, j, corr in high_corr_pairs[:5]:  # Show first 5\n",
    "                print(f\"  {self.feature_names[i][:30]}... ↔ {self.feature_names[j][:30]}...: r={corr:.3f}\")\n",
    "            if len(high_corr_pairs) > 5:\n",
    "                print(f\"  ... and {len(high_corr_pairs)-5} more\")\n",
    "        else:\n",
    "            print(\"No highly redundant features (|r|>0.9)\")\n",
    "\n",
    "    def diagnostic_report(self, survival_data=None):\n",
    "        \"\"\"\n",
    "        Run all diagnostics and generate a report\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"CLINICAL EMBEDDINGS DIAGNOSTIC REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        self.basic_statistics()\n",
    "        self.visualize_embeddings(survival_data)\n",
    "        self.cluster_analysis()\n",
    "        self.feature_importance_analysis(survival_data)\n",
    "        self.correlation_structure()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Overall assessment\n",
    "        issues = []\n",
    "\n",
    "        # Check 1: Feature count\n",
    "        if self.n_features < 10:\n",
    "            issues.append(\"Low feature count (<10)\")\n",
    "        elif self.n_features > 500:\n",
    "            issues.append(\"Very high feature count (>500), consider dimensionality reduction\")\n",
    "\n",
    "        # Check 2: Sparsity\n",
    "        sparsity = (self.X == 0).sum() / self.X.size\n",
    "        if sparsity > 0.8:\n",
    "            issues.append(f\"High sparsity ({sparsity:.1%})\")\n",
    "\n",
    "        # Check 3: Variance\n",
    "        if self.X.std() < 0.1:\n",
    "            issues.append(\"Low overall variance\")\n",
    "\n",
    "        # Check 4: Clustering\n",
    "        pca = PCA(n_components=2)\n",
    "        X_pca = pca.fit_transform(self.X)\n",
    "        if pca.explained_variance_ratio_.sum() < 0.3:\n",
    "            issues.append(f\"Low variance explained by first 2 PCs ({pca.explained_variance_ratio_.sum():.1%})\")\n",
    "\n",
    "        if issues:\n",
    "            print(\"POTENTIAL ISSUES FOUND:\")\n",
    "            for issue in issues:\n",
    "                print(f\"  - {issue}\")\n",
    "        else:\n",
    "            print(\"Embeddings look reasonable\")\n",
    "\n",
    "        print(f\"\\nFeature space: {self.n_features} dimensions\")\n",
    "        print(f\"Patient count: {self.n_patients}\")\n",
    "        print(f\"Dimensionality ratio: {self.n_patients / self.n_features:.1f} patients/feature\")\n",
    "\n",
    "        if self.n_patients / self.n_features < 5:\n",
    "            print(\"Warning: Low patients/feature ratio (<5), risk of overfitting\")\n",
    "\n",
    "        return {\n",
    "            'n_patients': self.n_patients,\n",
    "            'n_features': self.n_features,\n",
    "            'issues': issues,\n",
    "            'sparsity': sparsity,\n",
    "            'mean_variance': self.X.var(),\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage with your data\n",
    "def analyze_my_embeddings():\n",
    "    \"\"\"\n",
    "    Run complete analysis on your clinical embeddings\n",
    "    \"\"\"\n",
    "    # Initialize analyzer\n",
    "    print(\"Loading clinical embeddings...\")\n",
    "    analyzer = ClinicalEmbeddingAnalyzer(\"data/clinical_map.pkl\")\n",
    "\n",
    "    # Load survival labels from separate file\n",
    "    try:\n",
    "        with open(\"data/survival_labels.pkl\", \"rb\") as f:\n",
    "            survival_data = pickle.load(f)\n",
    "        print(f\"Loaded survival labels: {len(survival_data)} patients\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Warning: survival_labels.pkl not found. Trying to load from clinical_map.pkl...\")\n",
    "        # Fallback: check if labels are in the clinical_map file\n",
    "        with open(\"data/clinical_map.pkl\", \"rb\") as f:\n",
    "            clinical_data = pickle.load(f)\n",
    "\n",
    "        if \"survival_labels\" in clinical_data:\n",
    "            survival_data = clinical_data[\"survival_labels\"]\n",
    "            print(f\"Found survival labels in clinical_map: {len(survival_data)} patients\")\n",
    "        else:\n",
    "            print(\"No survival labels found. Analysis will proceed without survival coloring.\")\n",
    "            survival_data = None\n",
    "\n",
    "    report = analyzer.diagnostic_report(survival_data)\n",
    "    return report\n",
    "\n",
    "\n",
    "def quality_check(clinical_map_path: str = \"data/clinical_map.pkl\"):\n",
    "    with open(clinical_map_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    mapping = data[\"mapping\"]\n",
    "    feature_names = data[\"feature_names\"]\n",
    "\n",
    "    X = np.stack(list(mapping.values()))\n",
    "\n",
    "    print(\"QUICK QUALITY CHECK\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    checks = []\n",
    "\n",
    "    checks.append((\"Valid matrix\", X.shape[0] > 0 and X.shape[1] > 0))\n",
    "    checks.append((\"No NaN values\", np.isnan(X).sum() == 0))\n",
    "    n_features = X.shape[1]\n",
    "    checks.append((f\"Feature count ({n_features})\", 10 <= n_features <= 500))\n",
    "    value_range = X.max() - X.min()\n",
    "    checks.append((f\"Value range ({value_range:.2f})\", 0.1 < value_range < 100))\n",
    "    checks.append((\"Feature names\", len(feature_names) == n_features and\n",
    "                  all(isinstance(f, str) for f in feature_names)))\n",
    "\n",
    "    all_pass = True\n",
    "    for check_name, passed in checks:\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        print(f\"{status}: {check_name}\")\n",
    "        if not passed:\n",
    "            all_pass = False\n",
    "\n",
    "    if all_pass:\n",
    "        print(\"\\nAll checks passed! Embeddings look ready for model integration.\")\n",
    "    else:\n",
    "        print(\"\\nSome checks failed. Review the embeddings before integration.\")\n",
    "\n",
    "    return all_pass\n",
    "\n",
    "\n",
    "quality_ok = quality_check(\"../../PathomicFusion/data/TCGA_GBMLGG/clinical_map.pkl\")\n",
    "if quality_ok:\n",
    "    print(\"\\n\\nRunning full diagnostic analysis...\")\n",
    "    report = analyze_my_embeddings()\n",
    "else:\n",
    "    print(\"\\nSkipping full analysis due to quality issues.\")"
   ],
   "id": "1d301cc487f21be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def check_pca_variance_cumulative(analyzer):\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    X = analyzer.X\n",
    "    pca_full = PCA()\n",
    "    pca_full.fit(X)\n",
    "    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "    pc_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "    pc_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "    pc_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'b-o')\n",
    "    plt.axhline(y=0.80, color='r', linestyle='--', alpha=0.7, label='80% variance')\n",
    "    plt.axhline(y=0.90, color='g', linestyle='--', alpha=0.7, label='90% variance')\n",
    "    plt.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label='95% variance')\n",
    "\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance')\n",
    "    plt.title('PCA: How many dimensions explain most variance?')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.text(pc_80, 0.80, f'{pc_80} PCs', ha='left', va='bottom')\n",
    "    plt.text(pc_90, 0.90, f'{pc_90} PCs', ha='left', va='bottom')\n",
    "    plt.text(pc_95, 0.95, f'{pc_95} PCs', ha='left', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"PCA Variance Analysis:\")\n",
    "    print(f\"   First 2 PCs explain: {cumulative_variance[1]:.1%} variance\")\n",
    "    print(f\"   Need {pc_80} PCs to explain 80% variance\")\n",
    "    print(f\"   Need {pc_90} PCs to explain 90% variance\")\n",
    "    print(f\"   Need {pc_95} PCs to explain 95% variance\")\n",
    "\n",
    "    intrinsic_dimensionality = pc_95\n",
    "    total_features = X.shape[1]\n",
    "\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    print(f\"   Your data has {total_features} features\")\n",
    "    print(f\"   Only {intrinsic_dimensionality} are needed for 95% information\")\n",
    "    print(f\"   Dimensionality reduction ratio: {intrinsic_dimensionality/total_features:.1%}\")\n",
    "\n",
    "    if intrinsic_dimensionality/total_features < 0.3:\n",
    "        print(\"Efficient encoding: You're capturing information efficiently!\")\n",
    "    else:\n",
    "        print(\"Consider feature selection/compression for efficiency\")\n",
    "\n",
    "    return cumulative_variance"
   ],
   "id": "be5807220ee5a72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def clinical_specific_checks(analyzer):\n",
    "    \"\"\"\n",
    "    More relevant checks for clinical embeddings\n",
    "    \"\"\"\n",
    "    X = analyzer.X\n",
    "    feature_names = analyzer.feature_names\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CLINICAL-SPECIFIC QUALITY CHECKS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    clinical_categories = {\n",
    "        'demographic': ['age', 'sex', 'race', 'ethnicity'],\n",
    "        'diagnosis': ['diagnosis', 'morphology', 'icd', 'classification'],\n",
    "        'treatment': ['surgery', 'radiation', 'chemo', 'dexamethasone'],\n",
    "        'outcome': ['survival', 'progression', 'kps', 'vital'],\n",
    "        'temporal': ['days_to', 'first_', 'time']\n",
    "    }\n",
    "\n",
    "    print(\"\\n1. Feature Category Distribution:\")\n",
    "    for category, keywords in clinical_categories.items():\n",
    "        count = sum(any(keyword in name.lower() for keyword in keywords)\n",
    "                   for name in feature_names)\n",
    "        print(f\"   {category.title():15}: {count:3d} features\")\n",
    "\n",
    "    print(\"\\n2. Feature Value Ranges (sample of 10 features):\")\n",
    "    for i in np.random.choice(len(feature_names), 10, replace=False):\n",
    "        feat = X[:, i]\n",
    "        name = feature_names[i]\n",
    "        if len(name) > 30:\n",
    "            name = name[:27] + \"...\"\n",
    "        print(f\"   {name:30}: [{feat.min():7.3f}, {feat.max():7.3f}] mean={feat.mean():7.3f}\")\n",
    "\n",
    "    print(\"\\n3. Patient-level Statistics:\")\n",
    "    patient_means = X.mean(axis=1)\n",
    "    patient_stds = X.std(axis=1)\n",
    "    print(f\"   Mean feature value per patient: {patient_means.mean():.3f} ± {patient_means.std():.3f}\")\n",
    "    print(f\"   Feature std per patient: {patient_stds.mean():.3f} ± {patient_stds.std():.3f}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def check_for_model_readiness(analyzer):\n",
    "    X = analyzer.X\n",
    "    n_patients, n_features = X.shape\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL READINESS CHECKLIST\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    checklist = []\n",
    "\n",
    "    patients_per_feature = n_patients / n_features\n",
    "    checklist.append({\n",
    "        'item': f\"Patients/feature ratio ({patients_per_feature:.1f})\",\n",
    "        'status': patients_per_feature >= 5,\n",
    "        'message': '≥5 is good, <2 risks overfitting' if patients_per_feature < 2 else 'Good!'\n",
    "    })\n",
    "\n",
    "    has_nan = np.isnan(X).any()\n",
    "    checklist.append({\n",
    "        'item': \"No NaN values\",\n",
    "        'status': not has_nan,\n",
    "        'message': 'All values are numerical' if not has_nan else 'Contains NaN - needs imputation'\n",
    "    })\n",
    "\n",
    "    data_range = X.max() - X.min()\n",
    "    checklist.append({\n",
    "        'item': f\"Feature scale ({data_range:.1f} range)\",\n",
    "        'status': 0.1 < data_range < 100,\n",
    "        'message': f'Range {data_range:.1f} is appropriate' if 0.1 < data_range < 100 else 'Consider rescaling'\n",
    "    })\n",
    "\n",
    "    sparsity = (X == 0).sum() / X.size\n",
    "    checklist.append({\n",
    "        'item': f\"Feature sparsity ({sparsity:.1%})\",\n",
    "        'status': sparsity < 0.9,\n",
    "        'message': 'Reasonable information density' if sparsity < 0.9 else 'Very sparse - might need different encoding'\n",
    "    })\n",
    "\n",
    "    feature_vars = X.var(axis=0)\n",
    "    low_var_features = (feature_vars < 0.01).sum()\n",
    "    checklist.append({\n",
    "        'item': f\"Low-variance features ({low_var_features})\",\n",
    "        'status': low_var_features / n_features < 0.1,\n",
    "        'message': f'{low_var_features} features with <0.01 variance' if low_var_features > 0 else 'All features have reasonable variance'\n",
    "    })\n",
    "\n",
    "    print(\"\\nChecklist Results:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    all_good = True\n",
    "    for check in checklist:\n",
    "        status = \"PASS\" if check['status'] else \"FAIL\"\n",
    "        print(f\"{status} - {check['item']}\")\n",
    "        print(f\"     Note: {check['message']}\")\n",
    "        if not check['status']:\n",
    "            all_good = False\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    if all_good:\n",
    "        print(\"Embeddings are ready to be integrated.\")\n",
    "    else:\n",
    "        print(\"Some issues need attention before integration.\")\n",
    "\n",
    "    return all_good"
   ],
   "id": "885b3f0c9fd651a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "analyzer = ClinicalEmbeddingAnalyzer(\"../../PathomicFusion/data/TCGA_GBMLGG/clinical_map.pkl\")\n",
    "check_pca_variance_cumulative(analyzer)\n",
    "clinical_specific_checks(analyzer)\n",
    "is_ready = check_for_model_readiness(analyzer)"
   ],
   "id": "ce65ccd5bf085e5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def remove_low_variance_features(clinical_map_path: str = \"data/clinical_map.pkl\",\n",
    "                                 variance_threshold: float = 0.01,\n",
    "                                 output_path: str = \"data/clinical_map_filtered.pkl\"):\n",
    "    \"\"\"\n",
    "    Remove low-variance features from clinical embeddings\n",
    "\n",
    "    Args:\n",
    "        variance_threshold: Minimum variance to keep a feature (default 0.01)\n",
    "        output_path: Where to save the filtered embeddings\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    import numpy as np\n",
    "\n",
    "    with open(clinical_map_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    mapping = data[\"mapping\"]\n",
    "    feature_names = data[\"feature_names\"]\n",
    "    patient_ids = list(mapping.keys())\n",
    "    X = np.stack([mapping[pid] for pid in patient_ids])\n",
    "    variances = X.var(axis=0)\n",
    "\n",
    "    # Identify low-variance features\n",
    "    low_var_mask = variances < variance_threshold\n",
    "    low_var_indices = np.where(low_var_mask)[0]\n",
    "\n",
    "    print(f\"Original: {X.shape[1]} features\")\n",
    "    print(f\"Low-variance features (<{variance_threshold}): {low_var_mask.sum()}\")\n",
    "\n",
    "    if low_var_mask.sum() > 0:\n",
    "        print(\"\\nLow-variance features to remove:\")\n",
    "        for idx in low_var_indices:\n",
    "            name = feature_names[idx]\n",
    "            var = variances[idx]\n",
    "            if len(name) > 50:\n",
    "                name = name[:47] + \"...\"\n",
    "            print(f\"  {name:50} variance={var:.6f}\")\n",
    "\n",
    "    # Keep only high-variance features\n",
    "    high_var_mask = ~low_var_mask\n",
    "    X_filtered = X[:, high_var_mask]\n",
    "    feature_names_filtered = [feature_names[i] for i in range(len(feature_names))\n",
    "                             if high_var_mask[i]]\n",
    "\n",
    "    mapping_filtered = {}\n",
    "    for i, pid in enumerate(patient_ids):\n",
    "        mapping_filtered[pid] = X_filtered[i]\n",
    "\n",
    "    print(f\"\\nFiltered: {X_filtered.shape[1]} features\")\n",
    "    print(f\"Removed: {low_var_mask.sum()} features ({low_var_mask.sum()/X.shape[1]:.1%})\")\n",
    "\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"mapping\": mapping_filtered,\n",
    "            \"feature_names\": feature_names_filtered,\n",
    "            \"patient_ids\": patient_ids,\n",
    "            \"n_features\": X_filtered.shape[1],\n",
    "            \"removed_features\": [feature_names[i] for i in low_var_indices] if low_var_indices.size > 0 else [],\n",
    "            \"variance_threshold\": variance_threshold\n",
    "        }, f)\n",
    "\n",
    "    print(f\"\\nSaved filtered embeddings to: {output_path}\")\n",
    "    return mapping_filtered, feature_names_filtered\n",
    "\n",
    "mapping_filtered, features_filtered = remove_low_variance_features()"
   ],
   "id": "8343bcf2df685199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Verification\n",
    "analyzer_fixed = ClinicalEmbeddingAnalyzer(\"../../PathomicFusion/data/TCGA_GBMLGG/clinical_map_filtered.pkl\")\n",
    "is_ready = check_for_model_readiness(analyzer_fixed)"
   ],
   "id": "3663ecb3cc1b9e88"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
